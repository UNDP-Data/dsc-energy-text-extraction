{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69dba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "# from urllib.parse import urlparse   \n",
    "import numpy as np\n",
    "# To read the PDF\n",
    "import PyPDF2\n",
    "# To analyze the PDF layout and extract text\n",
    "from pdfminer.high_level import extract_pages, extract_text\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTRect, LTFigure\n",
    "# To extract text from tables in PDF\n",
    "import pdfplumber\n",
    "import spacy\n",
    "# To extract the images from the PDFs\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "# To perform OCR to extract text from images \n",
    "# To remove the additional created files\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "import textwrap\n",
    "from selenium import webdriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89da5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b48197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2601a8c",
   "metadata": {},
   "source": [
    "<h3>Download Text Contents - SEH</h3>\n",
    "<p>Use this to scrap needed text from a web source for SEH</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714bc5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link(row):\n",
    "    # Check if 'Link' column is available in the DataFrame\n",
    "    if 'Link' in row.index:\n",
    "        link = row['Link']\n",
    "    else:\n",
    "        link = None\n",
    "    \n",
    "    # Function to extract link from 'Title (+link)*' column\n",
    "    if 'Title (+link)*' in row.index and pd.notna(row['Title (+link)*']):\n",
    "        title_link_str = row['Title (+link)*']\n",
    "    else:\n",
    "        title_link_str = \"\"\n",
    "    \n",
    "    if isinstance(link, str):\n",
    "        return link\n",
    "    else:\n",
    "        # If 'Link' is not a string, extract link from 'Title (+link)*' column\n",
    "        match = re.search(r'\\((.*?)\\)', title_link_str)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7c85400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_webpage(metadata_file_list, folder_names):\n",
    "    for metadata_file_index, metadata_file in enumerate(metadata_file_list):\n",
    "        metadata_df = pd.read_excel(f'../01_Input/00_Metadata/{metadata_file}')\n",
    "        metadata_df['Link'] = metadata_df.apply(extract_link, axis=1)  # Apply extract_link function\n",
    "        metadata_df['Scraped'] = 'No'  # Add 'Scraped' column to indicate whether a page has been scraped\n",
    "        \n",
    "        for folder_name_index, folder_name in enumerate(folder_names):\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "            \n",
    "            if metadata_file_index == folder_name_index:\n",
    "                for index, row in metadata_df.iterrows():\n",
    "                    if row['Scraped'] == 'Yes':\n",
    "                        continue  # Skip if already scraped\n",
    "                    \n",
    "                    if ('Link' in row.index and pd.notna(row['Link'])) or ('Title (+link)*' in row.index and pd.notna(row['Title (+link)*'])):\n",
    "                        # Get the URL to scrape\n",
    "                        web_link = row['Link'] if 'Link' in row.index and pd.notna(row['Link']) else row['Title (+link)*']\n",
    "                        # Extract the file extension\n",
    "                        file_extension = os.path.splitext(web_link)[1]\n",
    "\n",
    "                        # Check if the file extension is .pdf\n",
    "                        if file_extension.lower() != '.pdf':\n",
    "\n",
    "                            try:\n",
    "                                headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "                                response = requests.get(web_link, headers=headers, allow_redirects=True, timeout=10)\n",
    "                                \n",
    "                                title = row['Title']\n",
    "                                # print(web_link)\n",
    "                                # print(response.status_code)\n",
    "                                print('______________')\n",
    "                                if response.status_code == 200:\n",
    "                                    # Parse the HTML\n",
    "                                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                                    # Extract text from specific HTML elements\n",
    "                                    text = soup.get_text(separator=\"\\n\")\n",
    "                                    # Clean up the text\n",
    "                                    text = re.sub(r'\\n+', '\\n', text)  # Remove consecutive newlines\n",
    "                                    text = text.strip()  # Remove leading and trailing whitespace\n",
    "                                    text = textwrap.fill(text, width=80)  # Wrap text to 80 characters per line\n",
    "                                    \n",
    "                                    # Save extracted text in a text file\n",
    "                                    # filename = f\"{title.replace('/', '_')}_{index}.txt\"\n",
    "                                    filename = f\"{str(title).replace('/', '_')}.txt\"  # Convert title to string and replace '/'\n",
    "\n",
    "                                    file_path = os.path.join(folder_name, filename)\n",
    "                                    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                                        file.write(text)\n",
    "                                    \n",
    "                                    print(f\"Content saved to: {file_path}\")\n",
    "\n",
    "                                    # Update 'Scraped' column in metadata DataFrame\n",
    "                                    metadata_df.at[index, 'Scraped'] = 'Yes'\n",
    "\n",
    "                                elif response.status_code == 403 :\n",
    "                                    \n",
    "                                    print(\"Download pdf from scihub \")\n",
    "                                    print(web_link)\n",
    "\n",
    "                                    # Send a GET request to the URL and retrieve the HTML content\n",
    "                                    response = requests.get(web_link)\n",
    "                                    html_content = response.content\n",
    "                                    # Parse the HTML content using BeautifulSoup\n",
    "                                    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                                        \n",
    "                                    # Find the div element with id \"buttons\"\n",
    "                                    buttons_div = soup.find('div', {'id': 'buttons'})\n",
    "\n",
    "                                    # Check if the div element is found\n",
    "                                    if buttons_div:\n",
    "                                        # Find the button element within the div\n",
    "                                        pdf_button = buttons_div.find('button')\n",
    "\n",
    "                                        if pdf_button:\n",
    "                                            # Extract the value of the \"onclick\" attribute\n",
    "                                            onclick_value = pdf_button.get('onclick')\n",
    "\n",
    "                                            # Extract the PDF link from the onclick attribute value\n",
    "                                            pdf_link = onclick_value.split(\"'\")[1]\n",
    "\n",
    "                                            print(\"PDF Link:\", pdf_link)\n",
    "                                        else:\n",
    "                                            print(\"No button element found within the div.\")\n",
    "                                    else:\n",
    "                                        print(\"No div element with id 'buttons' found.\")\n",
    "\n",
    "\n",
    "                                else:\n",
    "                                    print(\"Failed to fetch the page.\")\n",
    "                            except requests.exceptions.RequestException as e:\n",
    "                                print(f\"Error fetching the page: {e}\")\n",
    "\n",
    "                        else :\n",
    "                            print('is a pdf. Scrape differently...') \n",
    "                            #Todo:: add pdf extract function here.\n",
    "\n",
    "                break  # Break after processing the corresponding folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of metadata Excel files\n",
    "metadata_file_list = ['(Old) Mapping Journal articles - UNV Sustainable Energy Hub.xlsx',\n",
    "                      'Energy Document Catalogue.xlsx',\n",
    "                      'Energy Journal Article Compilation.xlsx',\n",
    "                      'UNDP Energy CO Press Releases.xlsx']\n",
    "\n",
    "# Define folder names array\n",
    "publication_folder = \"../02_Output/00_Extracted-Text/02_Publications/UNDP\"\n",
    "folder_names = [\n",
    "                \"../02_Output/00_Extracted-Text/00_SEH\",\n",
    "                publication_folder,\n",
    "                publication_folder,\n",
    "                publication_folder]\n",
    "\n",
    "\n",
    "\n",
    "# metadata_file_list = ['(Old) Mapping Journal articles - UNV Sustainable Energy Hub.xlsx']\n",
    "# folder_names = [\"../02_Output/00_Extracted-Text/00_SEH\"]\n",
    "# Call scrap_webpage function\n",
    "scrap_webpage(metadata_file_list, folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26fe72",
   "metadata": {},
   "source": [
    "<h3> Extract National Documents & United Resolution PDFs (Open Metadata) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcc4fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Metadata CSV\n",
    "metadata_df = pd.read_excel('../01_Input/00_Metadata/National Energy Document Compilation.xlsx')\n",
    "metadata_df=metadata_df[metadata_df[\"Exists?\"]==\"Y\"]\n",
    "\n",
    "\n",
    "#Load Un Resolutions data\n",
    "metadata_df_un_resolution = pd.read_excel('../01_Input/00_Metadata/UN Resolutions.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5f038",
   "metadata": {},
   "source": [
    "### Clean the Dataframe\n",
    "- Some rows in the .xls df don't have pdf documents. They should be skipped for now to avoid NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ae4fb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resolution No</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A/C.2/77/L.70_x000D_</td>\n",
       "      <td>https://undocs.org/A/C.2/77/L.70_x000D_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A/RES/75/211</td>\n",
       "      <td>https://undocs.org/A/RES/75/211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A/78/201</td>\n",
       "      <td>https://undocs.org/A/78/201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A/RES/65/151</td>\n",
       "      <td>https://undocs.org/A/RES/65/151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A/RES/67/215</td>\n",
       "      <td>https://undocs.org/A/RES/67/215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A/RES/69/225</td>\n",
       "      <td>https://undocs.org/A/RES/69/225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A/67/150</td>\n",
       "      <td>https://undocs.org/A/67/150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A/69/395</td>\n",
       "      <td>https://undocs.org/A/69/395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A/RES/63/210</td>\n",
       "      <td>https://undocs.org/A/RES/63/210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A/RES/66/206</td>\n",
       "      <td>https://undocs.org/A/RES/66/206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A/RES/66/110</td>\n",
       "      <td>https://undocs.org/A/RES/66/110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A/RES/67/263</td>\n",
       "      <td>https://undocs.org/A/RES/67/263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A/RES/71/233</td>\n",
       "      <td>https://undocs.org/A/RES/71/233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A/RES/72/224</td>\n",
       "      <td>https://undocs.org/A/RES/72/224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A/RES/73/236</td>\n",
       "      <td>https://undocs.org/A/RES/73/236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A/RES/74/225</td>\n",
       "      <td>https://undocs.org/A/RES/74/225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A/RES/76/210</td>\n",
       "      <td>https://undocs.org/A/RES/76/210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A/77/L.93</td>\n",
       "      <td>https://undocs.org/A/77/L.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A/77/443/ADD.9</td>\n",
       "      <td>https://undocs.org/A/77/443/ADD.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A/78/461/ADD.8</td>\n",
       "      <td>https://undocs.org/A/78/461/ADD.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A/78/461</td>\n",
       "      <td>https://undocs.org/A/78/461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A/RES/64/206</td>\n",
       "      <td>https://undocs.org/A/RES/64/206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A/RES/63/210</td>\n",
       "      <td>https://undocs.org/A/RES/63/210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A/RES/62/197</td>\n",
       "      <td>https://undocs.org/A/RES/62/197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Resolution No                                     Link\n",
       "0   A/C.2/77/L.70_x000D_  https://undocs.org/A/C.2/77/L.70_x000D_\n",
       "1           A/RES/75/211          https://undocs.org/A/RES/75/211\n",
       "2               A/78/201              https://undocs.org/A/78/201\n",
       "3           A/RES/65/151          https://undocs.org/A/RES/65/151\n",
       "4           A/RES/67/215          https://undocs.org/A/RES/67/215\n",
       "5          A/RES/69/225          https://undocs.org/A/RES/69/225 \n",
       "6               A/67/150              https://undocs.org/A/67/150\n",
       "7               A/69/395              https://undocs.org/A/69/395\n",
       "8           A/RES/63/210          https://undocs.org/A/RES/63/210\n",
       "9           A/RES/66/206          https://undocs.org/A/RES/66/206\n",
       "10         A/RES/66/110          https://undocs.org/A/RES/66/110 \n",
       "11         A/RES/67/263          https://undocs.org/A/RES/67/263 \n",
       "12          A/RES/71/233          https://undocs.org/A/RES/71/233\n",
       "13          A/RES/72/224          https://undocs.org/A/RES/72/224\n",
       "14          A/RES/73/236          https://undocs.org/A/RES/73/236\n",
       "15          A/RES/74/225          https://undocs.org/A/RES/74/225\n",
       "16          A/RES/76/210          https://undocs.org/A/RES/76/210\n",
       "17             A/77/L.93             https://undocs.org/A/77/L.93\n",
       "18        A/77/443/ADD.9        https://undocs.org/A/77/443/ADD.9\n",
       "19        A/78/461/ADD.8        https://undocs.org/A/78/461/ADD.8\n",
       "20              A/78/461              https://undocs.org/A/78/461\n",
       "21          A/RES/64/206          https://undocs.org/A/RES/64/206\n",
       "22          A/RES/63/210          https://undocs.org/A/RES/63/210\n",
       "23          A/RES/62/197          https://undocs.org/A/RES/62/197"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If the cell doesn't have a pdf Link, skip it \n",
    "metadata_df = metadata_df.dropna(subset=['Link'])\n",
    "metadata_df.dropna(subset=['Link'], inplace=True)\n",
    "metadata_d = metadata_df.head(100)\n",
    "\n",
    "\n",
    "#Do same for UN Resolution\n",
    "metadata_df_un_resolution = metadata_df_un_resolution.dropna(subset=['Link'])\n",
    "metadata_df_un_resolution.dropna(subset=['Link'], inplace=True)\n",
    "# metadata_df_un_resolution = metadata_df_un_resolution.head(25)\n",
    "metadata_df_un_resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df116404",
   "metadata": {},
   "source": [
    "<h3> Donwload PDFs </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56445f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download a pdf from a url to a path if is exists\n",
    "def download_document(pdf_url,pdf_path):\n",
    "    \n",
    "    # Code to download the document from the URL\n",
    "    # Handle different formats (PDF, HTML, etc.)\n",
    "   \n",
    "    try:\n",
    "        response = requests.get(pdf_url,timeout=8)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            #content_disposition = response.headers.get(\"content-disposition\")\n",
    "\n",
    "            with open(pdf_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            print(f\"{pdf_path} successfully downloaded document with path: {pdf_url}\")\n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(f\"Code 200, failed to downloaded document with path: {pdf_url}\")\n",
    "            return \"None\"\n",
    "    except:\n",
    "        print(f\"Failure to download document with path: {pdf_url}\")\n",
    "        return \"None\"\n",
    "    \n",
    "#Example Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74d2d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download missing document\n",
    "def download_document_if_missing(row,pdf_path):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        ##Download document if the pdf is not already present\n",
    "        pdf_url = row['Link']\n",
    "        if not pd.isna(pdf_url):\n",
    "            download_document(pdf_url,pdf_path)\n",
    "        else:\n",
    "            print(pdf_path,\"no pdf or link\")\n",
    "    else:\n",
    "        print(pdf_path,\"already exists\")\n",
    "        \n",
    "#Example Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4c7c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the download function for all\n",
    "def download_all(metadata_df,missing_only=True):\n",
    "    \n",
    "    for index, row in metadata_df.iterrows():\n",
    "\n",
    "        #nationals \n",
    "        folder=\"../01_Input/01_PDFs/03_National/\"+row[\"Code\"].split(\"-\")[0]\n",
    "        # Check if the directory already exists\n",
    "        if not os.path.exists(folder):\n",
    "            # Create the directory\n",
    "            os.makedirs(folder)\n",
    "            print(folder+\"directory created!\")\n",
    "\n",
    "        #nationals\n",
    "        pdf_path=folder+'/'+row[\"Code\"]+\".pdf\"\n",
    "\n",
    "        #un resolution\n",
    "        pdf_path=folder+'/'+row[\"Resolution No\"]+\".pdf\"\n",
    "\n",
    "        download_document_if_missing(row,pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "687caadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the download function for all\n",
    "def download_all_un_resolution(metadata_df,missing_only=True):\n",
    "    \n",
    "    for index, row in metadata_df.iterrows():\n",
    "\n",
    "        folder=\"../01_Input/01_PDFs/05_UN Resolutions/\"+row[\"Resolution No\"].split(\"/\")[0]\n",
    "        pdf_link = row['Link']\n",
    "\n",
    "        # Initialize a webdriver (make sure you have chromedriver or geckodriver installed and in your PATH)\n",
    "        driver = webdriver.Chrome()  # Or specify the path to your webdriver executable\n",
    "        # Open the webpage\n",
    "        driver.get(pdf_link)\n",
    "\n",
    "        # Get the page source after the JavaScript has been executed\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Find the link element with inner text \"English\"\n",
    "        link = soup.find(\"a\", text=\"English\")\n",
    "        if link:\n",
    "            # Extract the URL from the href attribute\n",
    "            url = link[\"href\"]\n",
    "            print(\"URL:\", url)\n",
    "             # Initialize a webdriver (make sure you have chromedriver or geckodriver installed and in your PATH)\n",
    "            driver = webdriver.Chrome()  # Or specify the path to your webdriver executable\n",
    "\n",
    "            # Open the URL in the browser\n",
    "            driver.get(url)\n",
    "\n",
    "            # Give some time for the download to start\n",
    "            time.sleep(15)  # Adjust this time as needed\n",
    "            driver.quit()\n",
    "\n",
    "        else:\n",
    "            print(\"Link not found.\")\n",
    "        # Close the browser\n",
    "\n",
    "       \n",
    "        # Check if the directory already exists\n",
    "        if not os.path.exists(folder):\n",
    "            # Create the directory\n",
    "            os.makedirs(folder)\n",
    "            print(folder+\"directory created!\")\n",
    "        #un resolution\n",
    "        pdf_path=folder+'/'+row[\"Resolution No\"]+\".pdf\"\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Run the download pipeline for all\n",
    "# download_all(metadata_df,missing_only=True)\n",
    "# download_all_un_resolution(metadata_df_un_resolution,missing_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076cf3c6",
   "metadata": {},
   "source": [
    "<h1> Text Extraction (PDF to text) </h1>\n",
    "<pre>The text extraction flow is modified to extract in details\n",
    "  - Table of content\n",
    "  - Clean Page texts\n",
    "  - Tables\n",
    "  - Text from Images\n",
    "  - Images/diagrams\n",
    "  </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cc99070",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_extraction(element):\n",
    "    # Extracting the text from the in-line text element\n",
    "    line_text = element.get_text()\n",
    "    \n",
    "    # Find the formats of the text\n",
    "    # Initialize the list with all the formats that appeared in the line of text\n",
    "    line_formats = []\n",
    "    for text_line in element:\n",
    "        if isinstance(text_line, LTTextContainer):\n",
    "            # Iterating through each character in the line of text\n",
    "            for character in text_line:\n",
    "                if isinstance(character, LTChar):\n",
    "                    # Append the font name of the character\n",
    "                    line_formats.append(character.fontname)\n",
    "                    # Append the font size of the character\n",
    "                    line_formats.append(character.size)\n",
    "    # Find the unique font sizes and names in the line\n",
    "    format_per_line = list(set(line_formats))\n",
    "    \n",
    "    # Return a tuple with the text in each line along with its format\n",
    "    return (line_text, format_per_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8e56ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting tables from the page\n",
    "\n",
    "def extract_table(pdf_path, page_num, table_num):\n",
    "    # Open the pdf file\n",
    "    pdf = pdfplumber.open(pdf_path)\n",
    "    # Find the examined page\n",
    "    table_page = pdf.pages[page_num]\n",
    "    # Extract the appropriate table\n",
    "    table = table_page.extract_tables()[table_num]\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert table into the appropriate format\n",
    "def table_converter(table):\n",
    "    table_string = ''\n",
    "    # Iterate through each row of the table\n",
    "    for row_num in range(len(table)):\n",
    "        row = table[row_num]\n",
    "        # Remove the line breaker from the wrapped texts\n",
    "        cleaned_row = [item.replace('\\n', ' ') if item is not None and '\\n' in item else 'None' if item is None else item for item in row]\n",
    "        # Convert the table into a string \n",
    "        table_string+=('|'+'|'.join(cleaned_row)+'|'+'\\n')\n",
    "    # Removing the last line break\n",
    "    table_string = table_string[:-1]\n",
    "    return table_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b93292f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify table of contents in text and format it properlly\n",
    "def clean_text_toc(text_content):\n",
    "    # Define a regular expression pattern to match the table of contents\n",
    "    toc_pattern = re.compile(r'^\\s*(\\w+\\.\\s+.+?)\\s+(\\d+)\\s*', re.MULTILINE)\n",
    "\n",
    "    # Find all matches in the text content\n",
    "    matches = toc_pattern.findall(text_content)\n",
    "\n",
    "    # Format the matches as a table of contents\n",
    "    table_of_contents_title = \"Table of Contents:\\n\"\n",
    "    table_of_contents = \"\"\n",
    "\n",
    "    isToc = False\n",
    "    for item, page in matches:\n",
    "        # Check if page contains dots\n",
    "        if \". . . . . .\"  in text_content or \"........\" in text_content:\n",
    "            isToc = True\n",
    "            # Remove dots between the title and page number\n",
    "            formatted_item = item.replace(\". . . .\",\"\").replace(\"...\",\"\")\n",
    "            formatted_item = re.sub(r'\\s+', ' ', formatted_item).strip()\n",
    "            table_of_contents += f\"{formatted_item}: Page: {page}\\n\"\n",
    "        else : \n",
    "            continue  # Skip if it doesn't contain dots, not a table of content entry\n",
    "\n",
    "    if isToc : \n",
    "        return table_of_contents_title + table_of_contents\n",
    "    else : \n",
    "        return text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8074faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_content_text_extractor(extract_text_path,pdf_path):\n",
    "    \n",
    "    # print(pdf_path)\n",
    "    pdf_path=pdf_path\n",
    "    # create a PDF file object\n",
    "    pdfFileObj = open(pdf_path, 'rb')\n",
    "\n",
    "    # create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(pdfFileObj)\n",
    "    total_no_pages = len(list(pdf_reader.pages))\n",
    "\n",
    "    # Create the dictionary to extract text from each image\n",
    "    text_per_page = {}\n",
    "    # We extract the pages from the PDF\n",
    "    for pagenum, page in enumerate(extract_pages(pdf_path)):\n",
    "        # Initialize the variables needed for the text extraction from the page\n",
    "        pageObj = pdf_reader.pages[pagenum]\n",
    "        page_text = []\n",
    "        line_format = []\n",
    "        text_from_images = []\n",
    "        text_from_tables = []\n",
    "        page_content = []\n",
    "        # Initialize the number of the examined tables\n",
    "        table_num = 0\n",
    "        first_element= True\n",
    "        table_extraction_flag= False\n",
    "        # Open the pdf file\n",
    "        pdf = pdfplumber.open(pdf_path)\n",
    "        # Find the examined page\n",
    "        page_tables = pdf.pages[pagenum]\n",
    "        # Find the number of tables on the page\n",
    "        tables = page_tables.find_tables()\n",
    "\n",
    "\n",
    "        # Find all the elements\n",
    "        page_elements = [(element.y1, element) for element in page._objs]\n",
    "        # Sort all the elements as they appear in the page \n",
    "        page_elements.sort(key=lambda a: a[0], reverse=True)\n",
    "\n",
    "        # Find the elements that composed a page\n",
    "        for i,component in enumerate(page_elements):\n",
    "            # Extract the position of the top side of the element in the PDF\n",
    "            pos= component[0]\n",
    "            # Extract the element of the page layout\n",
    "            element = component[1]\n",
    "            \n",
    "            # Check if the element is a text element\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                # Check if the text appeared in a table\n",
    "                if table_extraction_flag == False:\n",
    "                    # Use the function to extract the text and format for each text element\n",
    "                    (line_text, format_per_line) = text_extraction(element)\n",
    "                    # Append the text of each line to the page text\n",
    "                    page_text.append(line_text)\n",
    "                    # Append the format for each line containing text\n",
    "                    line_format.append(format_per_line)\n",
    "                    page_content.append(line_text)\n",
    "                else:\n",
    "                    # Omit the text that appeared in a table\n",
    "                    pass\n",
    "            \n",
    "        # Create the key of the dictionary\n",
    "        dctkey = 'Page_'+str(pagenum)\n",
    "        # Add the list of list as the value of the page key\n",
    "        text_per_page[dctkey]= [page_text, line_format, text_from_images,text_from_tables, page_content]\n",
    "\n",
    "    # Closing the pdf file object\n",
    "    pdfFileObj.close()\n",
    "    \n",
    "    final_result = \"\"\n",
    "    # print(total_no_pages)\n",
    "    # Display the content of the page 0  for example\n",
    "    for i in range(0, total_no_pages):\n",
    "        page_text_content = ''.join(text_per_page[f\"Page_{i}\"][4])\n",
    "        final_result += clean_text_toc(page_text_content)\n",
    "       \n",
    "    return textwrap.fill(final_result, width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3eeaa6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract text from document\n",
    "\n",
    "def extract_text_from_pdf(extract_text_path,pdf_path):\n",
    "    try:\n",
    "        text = multimodal_content_text_extractor(extract_text_path,pdf_path) #extract_text(pdf_path)##imported function\n",
    "        ##export text\n",
    "        with open(extract_text_path, 'w') as file:\n",
    "            file.write(text)\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing PDF {pdf_path}: {str(e)}\")\n",
    "            print(\"invalid pdf\",pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fcdc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download missing documents\n",
    "def extract_text_if_missing(extract_text_path,pdf_path):\n",
    "    \n",
    "    if not os.path.exists(extract_text_path):\n",
    "        ##extract text if it is not already present\n",
    "        extract_text_from_pdf(extract_text_path,pdf_path)\n",
    "\n",
    "    else:\n",
    "        print(extract_text_path,\"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "500afc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the extract function for all\n",
    "def extract_all(metadata_df,missing_only=True):\n",
    "    \n",
    "    for index, row in metadata_df.iterrows():\n",
    "\n",
    "        folder=\"03_National/\"+row[\"Code\"].split(\"-\")[0]## update this for more docs\n",
    "        pdf_path=\"../01_Input/01_PDFs/\"+folder+'/'+row[\"Code\"]+\".pdf\"\n",
    "        extract_text_folder=\"../02_Output/00_Extracted-Text/\"+folder\n",
    "        \n",
    "        if os.path.exists(pdf_path):\n",
    "            \n",
    "            if not os.path.exists(extract_text_folder):\n",
    "                # Create the directory\n",
    "                os.makedirs(extract_text_folder)\n",
    "                print(extract_text_folder+\"directory created!\")\n",
    "\n",
    "            if missing_only:\n",
    "                extract_text_if_missing(extract_text_folder+'/'+row[\"Code\"]+\".txt\",pdf_path)\n",
    "            else:\n",
    "                extract_text_from_pdf(pdf_path)\n",
    "        else:\n",
    "            print(\"can't extract because no pdf\",row[\"Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8545b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the extract function for all\n",
    "def extract_all_un_resolution(metadata_df,missing_only=True):\n",
    "\n",
    "    # Define the folder path containing the PDF files\n",
    "    folder_path = \"../01_Input/01_PDFs/05_UN Resolutions\"\n",
    "    \n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a PDF file\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            # Construct the full path to the PDF file\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Define the path to save the extracted text\n",
    "            extract_text_path = f\"../02_Output/00_Extracted-Text/05_UN Resolutions/{filename}.txt\"\n",
    "            \n",
    "            # Call the extract_text_from_pdf function with the paths\n",
    "            extract_text_from_pdf(extract_text_path, pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a0924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_all(metadata_df) \n",
    "# extract_all_un_resolution(metadata_df_un_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6418734d",
   "metadata": {},
   "source": [
    "<h3> Text Cleaning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4983afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleaning \n",
    "\n",
    "def remove_noise(text):\n",
    " \n",
    "  # Remove lines with only whitespace characters\n",
    "    cleaned_text = re.sub(r\"^\\s*$\", \"\", text, flags=re.MULTILINE)\n",
    "    cleaned_text = cleaned_text.replace('\\uf0a7', ';')\n",
    "    cleaned_text = cleaned_text.replace('\\r', '\\n')\n",
    "    cleaned_text = re.sub(r\"\\n\", \" \", cleaned_text)  # remove newlines\n",
    "    # Remove multiple consecutive newlines\n",
    "    cleaned_text = re.sub(r\"\\n{2,}\", \"\\n\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)  # replace multiple spaces with a single space\n",
    "    # cleaned_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", cleaned_text)  # remove non-alphanumeric characters\n",
    "    # Remove lines with single letters\n",
    "    cleaned_text = re.sub(r\"\\b\\w\\b\", \"\", cleaned_text) # to remove the headers . not needed.\n",
    "    \n",
    "    # Remove numbers followed by a dot before a phrase to remove page issues\n",
    "    cleaned_text = re.sub(r\"\\b\\d+\\.\\s*\", \"\", cleaned_text).replace('.','')\n",
    "\n",
    "    cleaned_text = re.sub(r\"http\\S+|www\\S+|ftp\\S+\", \"\", cleaned_text) # remove urls\n",
    "    \n",
    "    #Remove headers taggins\n",
    "    cleaned_text = re.sub(r\"\\b\\d+-\\d+\\b|\\b/\\d+\\b|\\b\\w/\\w/\\w/\\b\", \"\", cleaned_text)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    # doc = nlp(cleaned_text)\n",
    "    \n",
    "    # # Keep sentences and join them together\n",
    "    # cleaned_text = \" \".join(sent.text for sent in doc.sents)\n",
    "    cleaned_text = textwrap.fill(cleaned_text, width=80) \n",
    " \n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_stopwords(text): \n",
    "    # nltk.download('stopwords') # only need to run this once\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text.split() if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "def convert_to_lowercase(text): # reduce words to their root forms\n",
    "    return text.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cbe32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(extract_text_path,clean_text_path):\n",
    "    \n",
    "    with open(extract_text_path, 'r') as file:\n",
    "        extract_text=file.read()\n",
    "    \n",
    "    # Remove unwanted characters, whitespace, etc.\n",
    "    # Regular expressions can be helpful here\n",
    "    # Additional cleaning steps as required\n",
    "    \n",
    "    clean_text = remove_noise(extract_text)\n",
    "    # clean_text = remove_punctuation(text)\n",
    "    # clean_text = remove_stopwords(text)\n",
    "    # clean_text = convert_to_lowercase(text)\n",
    "    # clean_text = lemmatize_text(text)\n",
    "    \n",
    "    with open(clean_text_path, 'w',  encoding=\"utf-8\") as file:\n",
    "        file.write(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1a01c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_if_missing(extract_text_path,clean_text_path):\n",
    "    \n",
    "    if not os.path.exists(clean_text_path):\n",
    "        ##extract text if it is not already present\n",
    "        clean_text(extract_text_path,clean_text_path)\n",
    "\n",
    "    else:\n",
    "        print(clean_text_path,\"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26044c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the clean function for all\n",
    "def clean_all(metadata_df,missing_only=True):\n",
    "    \n",
    "    for index, row in metadata_df.iterrows():\n",
    "\n",
    "        folder=\"03_National/\"+row[\"Code\"].split(\"-\")[0]## update this for more docs\n",
    "        \n",
    "        clean_text_folder=\"../02_Output/01_Cleaned-Text/\"+folder\n",
    "        extract_text_folder=\"../02_Output/00_Extracted-Text/\"+folder\n",
    "        \n",
    "        extract_text_path=extract_text_folder+'/'+row[\"Code\"]+\".txt\"\n",
    "        clean_text_path=clean_text_folder+'/'+row[\"Code\"]+\"-clean.txt\"\n",
    "        \n",
    "        if os.path.exists(extract_text_path):\n",
    "            \n",
    "            if not os.path.exists(clean_text_folder):\n",
    "                # Create the\n",
    "                #  directory\n",
    "                os.makedirs(clean_text_folder)\n",
    "                print(clean_text_folder+\"directory created!\")\n",
    "\n",
    "            if missing_only:\n",
    "                clean_text_if_missing(extract_text_path,clean_text_path)\n",
    "            else:\n",
    "                clean_text(extract_text_path,clean_text_path)\n",
    "        else:\n",
    "            print(\"can't clean because no extract\",row[\"Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8f1dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_all(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabbc59",
   "metadata": {},
   "source": [
    "<h3> convert final cleaned text and metadata to jsons </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32e7e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_json(json_path,clean_text_path,row, source_type):\n",
    "    print(f'current source type:: {source_type}')\n",
    "    with open(clean_text_path, 'r') as file:\n",
    "        text=file.read()\n",
    "    \n",
    "    if source_type == 'National':\n",
    "        document_json = {\n",
    "            'Code': str(row['Code']),\n",
    "            'Status': row['Status'],\n",
    "            'Country Name': row['Country Name'],\n",
    "            'Country Code': str(row['Country Code']),\n",
    "            'Category': row['Category'],\n",
    "            'KeyWord to Search': row['KeyWord to Search'],\n",
    "            'Document Title': row['Title'],\n",
    "            'Exists?': row['Exists?'],\n",
    "            'Category': row['Category'],\n",
    "            'Publication Date': str(row['Publication Date']),\n",
    "            'Publication Year': str(row['Publication Date']),\n",
    "            'Start Year': str(row['Start Year']),\n",
    "            'End Year': str(row['End Year']),\n",
    "            'Language': row['Language'],\n",
    "            'Link': row['Link'],\n",
    "            'Content': text  # Extracted text content from PDF\n",
    "        }\n",
    "\n",
    "    elif source_type == 'SEH':\n",
    "        document_json = {\n",
    "            'Code': '',\n",
    "            'Status': '',\n",
    "            'Country Name': str(row['Country*']) or '' ,\n",
    "            'Country Code': str(row['Country*']),\n",
    "            'Category': 'SEH',\n",
    "            'KeyWord to Search': '',\n",
    "            'Document Title': row['Title'],\n",
    "            'Exists?': 'Y',\n",
    "            'Publication Date': str(row['Day of mapping']),\n",
    "            'Publication Year': str(row['Year']),\n",
    "            'Start Year': str(row['Year']),\n",
    "            'End Year': str(row['Year']),\n",
    "            'Language': 'EN',\n",
    "            'Link': str(row['Title (+link)*']) or '',\n",
    "            'Content': text,  # Extracted text content from PDF\n",
    "\n",
    "            'Region': row['Region*'],\n",
    "            'Journal': row['Journal'],\n",
    "            'Authors': row['Authors/Institution*']\n",
    "\n",
    "        }\n",
    "        \n",
    "    elif source_type == 'Publications':\n",
    "        document_json = {\n",
    "            'Code': '',\n",
    "            'Status': '',\n",
    "            'Country Name': str(row.get('Country Name', '')),\n",
    "            'Country Code': str(row.get('Country Name', '')),\n",
    "            'Category': 'Publications',\n",
    "            'KeyWord to Search': str(row.get('tags', '')) or str(row.get('Tags', '')),\n",
    "            'Document Title': row['Title'],\n",
    "            'Exists?': 'Y',\n",
    "            'Publication Date': str(row.get('Publication Date', '')),\n",
    "            'Publication Year': '',\n",
    "            'Start Year': '',\n",
    "            'End Year': '',\n",
    "            'Language': 'EN',\n",
    "            'Link': row['Link'],\n",
    "            'Content': text,  # Extracted text content from PDF\n",
    "\n",
    "            'Description': str(row.get('Short Description', '')),\n",
    "            'Journal': str(row.get('Journal', '')),\n",
    "\n",
    "        }\n",
    "\n",
    "    elif source_type == 'UNResolution':\n",
    "        document_json = {\n",
    "            'Code': '',\n",
    "            'Status': '',\n",
    "            'Country Name': '',\n",
    "            'Country Code': '',\n",
    "            'Category': '',\n",
    "            'KeyWord to Search': '',\n",
    "            'Document Title': '',\n",
    "            'Exists?': 'Y',\n",
    "            'Publication Date': '',\n",
    "            'Publication Year': '',\n",
    "            'Start Year': '',\n",
    "            'End Year': '',\n",
    "            'Language': 'EN',\n",
    "            'Link': row['Link'],\n",
    "            'Content': text,  # Extracted text content from PDF\n",
    "\n",
    "            'Resolution No': str(row.get('Resolution No', '')),\n",
    "\n",
    "        }\n",
    "\n",
    "    else :\n",
    "        document_json={}\n",
    "\n",
    "\n",
    "    # Write the dictionary to a JSON file\n",
    "    with open(json_path, 'w') as jsonfile:\n",
    "        json.dump(document_json, jsonfile)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b95141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonify_if_missing(json_path,clean_text_path,row,source_type):\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        ##extract text if it is not already present\n",
    "        create_document_json(json_path,clean_text_path,row,source_type)\n",
    "    \n",
    "    else:\n",
    "        print(json_path,\"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d127fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonify_all(metadata_df,missing_only, source_type):\n",
    "    metadata_df=metadata_df.fillna(np.nan).replace([np.nan], [None])##fixes missing so json works\n",
    "    \n",
    "    for index, row in metadata_df.iterrows():\n",
    "\n",
    "        if source_type == 'National':\n",
    "           folder=\"03_National/\"+row[\"Code\"].split(\"-\")[0]## update this for more docs\n",
    "        \n",
    "        elif source_type == 'SEH':\n",
    "            folder=\"00_SEH\"## update this for more docs\n",
    "\n",
    "        elif source_type == 'UNResolution':\n",
    "            folder=\"05_UN Resolutions\"## update this for more docs\n",
    "      \n",
    "        elif source_type == 'Publications':\n",
    "            folder=\"02_Publications/UNDP\"## update this for more docs\n",
    "\n",
    "        else : \n",
    "             folder =''\n",
    "\n",
    "        clean_text_folder=\"../02_Output/01_Cleaned-Text/\"+folder ##later update this to the manual folder\n",
    "        json_folder=\"../02_Output/03_Pdf-Jsons/\"+folder\n",
    "        \n",
    "        if source_type == 'National':\n",
    "           clean_text_path=clean_text_folder+'/'+row[\"Code\"]+\"-clean.txt\"\n",
    "        \n",
    "        elif source_type == 'UNResolution':\n",
    "             clean_text_path=clean_text_folder+'/'+str(row[\"Resolution No\"]).replace('/',':')+\".pdf.txt\"\n",
    "\n",
    "        elif source_type == 'SEH':\n",
    "             if row[\"Title\"] != None:\n",
    "                 clean_text_path=clean_text_folder+'/'+row[\"Title\"]+\".txt\"\n",
    "\n",
    "        elif source_type == 'Publications':\n",
    "             if row[\"Title\"] != None:\n",
    "                clean_text_path=clean_text_folder+'/'+row[\"Title\"]+\".txt\"\n",
    "\n",
    "        else : \n",
    "             clean_text_path =''\n",
    "\n",
    "        if source_type == 'National':\n",
    "            json_path=json_folder+'/'+row[\"Code\"]+\".json\"\n",
    "\n",
    "        elif source_type == 'UNResolution':\n",
    "            json_path=json_folder+'/'+str(row[\"Resolution No\"]).replace('/',':')+\".json\"\n",
    "\n",
    "        elif source_type == 'SEH':\n",
    "             if row[\"Title\"] != None:\n",
    "                json_path=json_folder+'/'+row[\"Title\"]+\".json\"\n",
    "\n",
    "        elif source_type == 'Publications':\n",
    "             if row[\"Title\"] != None:\n",
    "                json_path=json_folder+'/'+row[\"Title\"]+\".json\"\n",
    "\n",
    "        else : \n",
    "             json_path =''\n",
    "        # print(clean_text_path)\n",
    "        if os.path.exists(clean_text_path):\n",
    "            # print(f\" Path found {clean_text_path}\")\n",
    "\n",
    "            if not os.path.exists(json_folder):\n",
    "                # Create the directory\n",
    "                os.makedirs(json_folder)\n",
    "                print(json_folder+\" directory created!\")\n",
    "            \n",
    "            if missing_only:\n",
    "                jsonify_if_missing(json_path,clean_text_path,row,source_type)\n",
    "            else:\n",
    "                create_document_json(json_path,clean_text_path,row,source_type)\n",
    "        else:\n",
    "            r=''\n",
    "            # if source_type == 'National':\n",
    "            #    print(\"can't jsonify because no clean text\",row[\"Code\"])\n",
    "            # elif source_type == 'SEH':\n",
    "            #      print(\"can't jsonify because no clean text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonify_all(metadata_df, True, 'National') #For National Documents\n",
    "\n",
    "# jsonify_all(metadata_df_un_resolution, True, 'UNResolution') #For United Nations Resolution\n",
    "\n",
    "# For publications and SEH\n",
    "metadata_file_list = ['(Old) Mapping Journal articles - UNV Sustainable Energy Hub.xlsx',\n",
    "                      'Energy Document Catalogue.xlsx',\n",
    "                      'Energy Journal Article Compilation.xlsx',\n",
    "                      'UNDP Energy CO Press Releases.xlsx']\n",
    "\n",
    "category = ['SEH',\n",
    "            'Publications',\n",
    "            'Publications',\n",
    "            'Publications']\n",
    "\n",
    "for metadata_file, category in zip(metadata_file_list, category):\n",
    "    print(f\"checking {metadata_file}\")\n",
    "    print(f\"checking {category}\")\n",
    "\n",
    "    metadata_df_file = pd.read_excel(f'../01_Input/00_Metadata/{metadata_file}')\n",
    "\n",
    "    jsonify_all(metadata_df_file, True, category)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bc7a8",
   "metadata": {},
   "source": [
    "<h3>Merge JSONs into one document.json for llm embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b2edb07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(folder_path, output_file):\n",
    "    # Initialize an empty list to store JSON data\n",
    "    all_data = []\n",
    "    \n",
    "    # Traverse through the folder and its subfolders\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            # Check if the file is a JSON file\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                # Read JSON data from the file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    try:\n",
    "                        json_data = json.load(f)\n",
    "                        all_data.append(json_data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file '{file_path}': {e}\")\n",
    "    \n",
    "    # Write the merged JSON data to the output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_data, f, indent=4)\n",
    "# Example usage:\n",
    "source_folder = '../02_Output/03_Pdf-Jsons'\n",
    "folder_path = '../02_Output'\n",
    "output_file = f'{folder_path}/04_Doc_Jsons_Merge/documents_all.json'\n",
    "merge_json_files(source_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b763a",
   "metadata": {},
   "source": [
    "-TODO\n",
    "\n",
    "<p> https://sci-hub.se/ for science direct pdfs </p>\n",
    "<p> Rename Create_ 04_Journal  folder </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
